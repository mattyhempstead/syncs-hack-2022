{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20719,"status":"ok","timestamp":1661644350197,"user":{"displayName":"Matty Hempstead","userId":"00658987594311765350"},"user_tz":-600},"id":"Gq-Outl2og-k","outputId":"07031b27-c051-487c-fdb3-436cdc50fbcf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sat Aug 27 23:52:09 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","\u001b[K     |████████████████████████████████| 112 kB 5.2 MB/s \n","\u001b[K     |████████████████████████████████| 120 kB 34.8 MB/s \n","\u001b[K     |████████████████████████████████| 4.7 MB 5.2 MB/s \n","\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s \n","\u001b[K     |████████████████████████████████| 6.6 MB 54.2 MB/s \n","\u001b[K     |████████████████████████████████| 1.6 MB 5.1 MB/s \n","\u001b[?25h"]}],"source":["## Pre-checks\n","# Check for GPU\n","!nvidia-smi\n","\n","# Dependencies\n","!pip install diffusers==0.2.4 --quiet\n","!pip install transformers scipy ftfy --quiet\n","!pip install \"ipywidgets>=7,<8\" --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":271},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1661644350198,"user":{"displayName":"Matty Hempstead","userId":"00658987594311765350"},"user_tz":-600},"id":"fMBWhhouolSr","outputId":"8b738f6f-c314-44ef-cc10-995ba36d600e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Login successful\n","Your token has been saved to /root/.huggingface/token\n","\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n","\n","git config --global credential.helper store\u001b[0m\n"]}],"source":["from google.colab import output\n","output.enable_custom_widget_manager()\n","\n","# Authenticate huggingface\n","#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\n","from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5uZP8zbgowmF"},"outputs":[],"source":["import torch\n","from torch import autocast\n","from diffusers import StableDiffusionPipeline\n","\n","# Install pipeline \n","# make sure you're logged in with `huggingface-cli login`\n","generator = torch.Generator(\"cuda\").manual_seed(1024)\n","pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True, generator=generator)  \n","pipe = pipe.to(\"cuda\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":4690,"status":"ok","timestamp":1661650107496,"user":{"displayName":"Matty Hempstead","userId":"00658987594311765350"},"user_tz":-600},"id":"AayZY0qQ31I2","outputId":"4f0079fc-b3fd-49fd-f503-5702b5462043"},"outputs":[{"name":"stdout","output_type":"stream","text":["https://c4a16yf7qzv-496ff2e9c6d22116-5000-colab.googleusercontent.com/\n"]}],"source":["# Get URL \n","from google.colab.output import eval_js\n","print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))\n","\n","!pip install pyngrok --quiet\n","from pyngrok import ngrok"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Ky7NZTJGRyZ"},"outputs":[],"source":["# import sys\n","\n","# print_old = print\n","# def print(*args, **kwargs):\n","#   print_old(*args, **kwargs, file=sys.stdout)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"n4ojoEPcs2aT","outputId":"94bc0fe3-cb68-4c4d-d4aa-5eb7169fed50"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tracking URL: NgrokTunnel: \"http://9b84-35-185-127-79.ngrok.io\" -> \"http://localhost:5000\"\n","http://9b84-35-185-127-79.ngrok.io/editing/image/process\n"," * Serving Flask app \"__main__\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug: * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n","INFO:werkzeug:127.0.0.1 - - [28/Aug/2022 01:32:50] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["index via GET\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [28/Aug/2022 01:32:50] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["index via GET\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [28/Aug/2022 01:32:53] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["index via GET\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [28/Aug/2022 01:33:28] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["index via GET\n"]}],"source":["import os\n","import sys\n","from threading import Thread\n","\n","\n","\n","# Get ngrok\n","def get_redirect():\n","    # Terminate open tunnels if exist\n","    ngrok.kill()\n","\n","    # Setting the authtoken (optional)\n","    # Get your authtoken from https://dashboard.ngrok.com/auth\n","    NGROK_AUTH_TOKEN = \"2DvhU3tgIpYxm9e66KhwfZGvCyI_7xJrKzuqczSLpAvq642HZ\"\n","    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n","\n","    # Open an HTTPs tunnel on port 5000 for http://localhost:5000\n","    public_url = ngrok.connect(5000, proto=\"http\", options={\"bind_tls\": True})\n","    print(\"Tracking URL:\", public_url)\n","    base = str(public_url).split('\"')[1]\n","    print(base + \"/editing/image/process\")\n","    return base\n","base_url = get_redirect()\n","\n","\n","\n","import threading\n","\n","\n","IMAGE_DIR = \"images/\"\n","\n","caption = \"\"\n","id2cap = {}\n","counter = 0\n","\n","\n","def get_image_path(image_id:str):\n","  return IMAGE_DIR + image_id + \".png\"\n","\n","# Returns True/False if image is done or not\n","def check_image(image_id:str):\n","  image_path = get_image_path(image_id)\n","  return os.path.exists(image_path)\n","\n","def get_image_url(image_id:str):\n","  global base_url\n","  return base_url + \"/image\" + \"?image_id=\" + image_id\n","\n","\n","\n","\n","def generate_image(image_id:str, prompt:str):\n","    file_path = IMAGE_DIR + image_id + \".png\"\n","\n","    with autocast(\"cuda\"):\n","        image = pipe(prompt)[\"sample\"][0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n","\n","        # Now to display an image you can do either save it such as:\n","        image_path = get_image_path(image_id)\n","        image.save(image_path)\n","        # image.save(\"tmp.png\")\n","\n","    # or if you're in a google colab you can directly display it with\n","    return\n","\n","\n","from flask import Flask, request, send_file, send_from_directory\n","import sys, json\n","\n","app = Flask(__name__)\n","\n","# Base URL\n","@app.route('/', methods=[\"GET\", \"POST\"])\n","def index():\n","    if request.method == \"POST\":\n","        print('index via POST', file=sys.stdout)\n","        return \"Incorrect URL\"\n","    else:\n","        print('index via GET', file=sys.stdout)\n","        return \"Incorrect URL\"\n","\n","\n","# Gets payload\n","def pull_payload(data, key):\n","    payload = json.loads(data.decode())\n","    settings = json.loads(payload['settings'])\n","    return settings[key]\n","\n","def get_response_started(image_id:str):\n","    data = {\n","        \"type\": \"SUCCESS\",\n","        \"id\": image_id\n","    }\n","\n","    response = app.response_class(\n","        response=json.dumps(data),\n","        status=200,\n","        mimetype='application/json'\n","    )\n","    return response\n","\n","def get_response_pending():\n","  data = {\n","      \"type\": \"SUCCESS\"\n","  }\n","  response = app.response_class(\n","      response=json.dumps(data),\n","      status=200,\n","      mimetype='application/json'\n","  )\n","  return response\n","\n","\n","def get_response_done(image_id:str):\n","    data = {\n","        \"type\": \"SUCCESS\",\n","        \"resource\": {\n","            \"url\": get_image_url(image_id),\n","            \"width\": 512,\n","            \"height\": 512,\n","            \"type\": \"PNG\"\n","        }\n","    }\n","\n","    response = app.response_class(\n","        response=json.dumps(data),\n","        status=200,\n","        mimetype='application/json'\n","    )\n","    return response\n","\n","import time\n","\n","# Retuns a new random unique image id\n","def get_image_id():\n","  import uuid\n","  return str(uuid.uuid4())\n","\n","\n","@app.route('/test', methods=['POST', 'GET'])\n","def test():\n","    return get_response_done()\n","\n","\n","\n","@app.route('/editing/image/process', methods=['POST'])\n","def edit():\n","    print('process via POST', time.time(), file=sys.stdout)\n","\n","    try:\n","        caption = pull_payload(request.data, \"caption\")\n","\n","        image_id = get_image_id()\n","\n","        # Start thread in background to generate image\n","        Thread(\n","            target=generate_image,\n","            args = (image_id, caption,)\n","        ).start()\n","\n","        return get_response_started(image_id)\n","\n","        # print(\"Caption:\", caption, type(caption), file=sys.stdout)\n","        # if caption == None or caption == \"\" or caption == \"null\":\n","        #     print(\"No caption\", file=sys.stdout)\n","        #     return get_response_done()\n","        # else:\n","            \n","            # beg = time.time()\n","\n","            # print(\"Generating:\", caption, beg, file=sys.stdout)\n","            # get_image(caption)\n","\n","            # print(time.time(), time.time() - beg, file=sys.stdout)\n","            # print(time.time(), time.time() - beg, file=sys.stdout)\n","            # return get_response()\n","    except:\n","        print(request.headers, file=sys.stdout)\n","        print(request.data, file=sys.stdout)\n","        return \"Unexpected Payload\"\n","\n","\n","@app.route('/editing/image/process/get', methods=['POST'])\n","def process():\n","    print('process/get via GET', time.time(), file=sys.stdout)\n","\n","    payload = json.loads(request.data.decode())\n","    print(\"payload\", payload)\n","    image_id = payload[\"id\"]\n","\n","    if check_image(image_id):\n","      return get_response_done(image_id)\n","    else:\n","      return get_response_pending()\n","\n","\n","\n","@app.route('/image', methods=['POST', 'GET'])\n","def serve_image():\n","  print('serving image', time.time(), file=sys.stdout)\n","\n","  image_id = request.args.get(\"image_id\")\n","  print(image_id)\n","  return send_from_directory(IMAGE_DIR, image_id+\".png\")\n","\n","\n","\n","app.run(host='0.0.0.0')\n","\n","\n","# if __name__ == '__main__':\n","#     w = Thread(target=worker)\n","#     w.start()\n","    \n","#     m = Thread(target=master)\n","#     m.start()\n","\n","#     w.join()\n","#     m.join()\n","    # app.run(debug=True, use_reloader=False, port=8000)\n","    # app.run(debug=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AuSHLLg1BHY6"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[{"file_id":"1lqGfB4SGlw6DG1m0hYAoIOs3-5FTmJzl","timestamp":1661581131736},{"file_id":"1fI9Xm5zEJm312copeY_abXsaiVoxYoTE","timestamp":1661579530530}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}